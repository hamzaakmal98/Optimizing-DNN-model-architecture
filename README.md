This paper presents an empirical study on the optimization of Deep Neural Network (DNN) architectures in TensorFlow for modeling complex functions, specifically aiming to determine the ideal combination of layers, neurons, and an extrinsic parameter ’a’ for the function y = arcsin(ax) + sin(10x). The study explores the effects of varying the number of layers and neurons, along with three distinct values of the extrinsic parameter ’a’, across twelve different datasets. Preliminary results are visualized using a DNN with two hidden layers and ten neurons with ReLU activation to fine-tune the number of epochs for subsequent analysis. The paper discusses the methodology for generating the datasets, the training process, and the implications of the findings for DNN architecture design in function modeling. A qualitative analysis of trends is presented followed by a rigorous quantitative analysis using covariance perturbation and random forest regressors to uncover the functional relationship between the parameters. The results prove the running hyptothesis that lowering the number of layers and increasing the number of neurons for a fully connected dense layer reduces the minimum convergent loss independent of the choice of function, activation or optimizer.
